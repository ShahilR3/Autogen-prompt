{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d6babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen in c:\\users\\shahi\\anaconda3\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: openai<1 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from pyautogen) (0.27.8)\n",
      "Requirement already satisfied: termcolor in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from pyautogen) (2.1.0)\n",
      "Requirement already satisfied: diskcache in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from pyautogen) (5.6.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from pyautogen) (1.0.0)\n",
      "Requirement already satisfied: flaml in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from pyautogen) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from openai<1->pyautogen) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from openai<1->pyautogen) (3.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from openai<1->pyautogen) (4.64.1)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from flaml->pyautogen) (1.23.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai<1->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai<1->pyautogen) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai<1->pyautogen) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai<1->pyautogen) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from aiohttp->openai<1->pyautogen) (6.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahi\\anaconda3\\lib\\site-packages (from tqdm->openai<1->pyautogen) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22d748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "\n",
      "Write python code to output GCD of numbers 10, 15 and 20 , and then store the code in a file\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCTO\u001b[0m (to user_proxy):\n",
      "\n",
      "Sure! Here's the Python code to calculate the GCD (Greatest Common Divisor) of numbers 10, 15, and 20:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def gcd(a, b):\n",
      "    while b != 0:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "\n",
      "numbers = [10, 15, 20]\n",
      "result = numbers[0]\n",
      "for i in range(1, len(numbers)):\n",
      "    result = gcd(result, numbers[i])\n",
      "\n",
      "print(\"GCD:\", result)\n",
      "```\n",
      "\n",
      "To store this code in a file, you can use the following Python code:\n",
      "\n",
      "```python\n",
      "code = '''\n",
      "import math\n",
      "\n",
      "def gcd(a, b):\n",
      "    while b != 0:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "\n",
      "numbers = [10, 15, 20]\n",
      "result = numbers[0]\n",
      "for i in range(1, len(numbers)):\n",
      "    result = gcd(result, numbers[i])\n",
      "\n",
      "print(\"GCD:\", result)\n",
      "'''\n",
      "\n",
      "with open('gcd.py', 'w') as file:\n",
      "    file.write(code)\n",
      "```\n",
      "\n",
      "This will create a file named \"gcd.py\" in the same directory as your Python script and write the code into it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "GCD: 5\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCTO\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The code executed successfully and the output is as expected. The GCD of numbers 10, 15, and 20 is indeed 5. If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "\n",
      "Change the code in the file you just created to instead output LCM OF numbers 10,15 and 20\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCTO\u001b[0m (to user_proxy):\n",
      "\n",
      "Sure, here's the updated code to calculate the LCM (Least Common Multiple) of numbers 10, 15, and 20:\n",
      "\n",
      "```python\n",
      "def calculate_lcm(numbers):\n",
      "    lcm = numbers[0]\n",
      "    for i in range(1, len(numbers)):\n",
      "        num = numbers[i]\n",
      "        lcm = (lcm * num) // calculate_gcd(lcm, num)\n",
      "    return lcm\n",
      "\n",
      "def calculate_gcd(a, b):\n",
      "    while b:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "\n",
      "numbers = [10, 15, 20]\n",
      "lcm = calculate_lcm(numbers)\n",
      "print(\"LCM of\", numbers, \"is\", lcm)\n",
      "```\n",
      "\n",
      "In this code, we define two functions: `calculate_lcm` and `calculate_gcd`. The `calculate_lcm` function takes a list of numbers as input and iteratively calculates the LCM using the formula `(lcm * num) // calculate_gcd(lcm, num)`. The `calculate_gcd` function calculates the Greatest Common Divisor (GCD) using the Euclidean algorithm.\n",
      "\n",
      "We then create a list `numbers` with the numbers 10, 15, and 20. We pass this list to the `calculate_lcm` function to calculate the LCM and store it in the variable `lcm`. Finally, we print the result as \"LCM of [10, 15, 20] is [lcm]\".\n",
      "\n",
      "Please note that you may need to adjust the code if you are using a different programming language.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n",
      "SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "LCM of [10, 15, 20] is 60\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCTO\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The code executed successfully and the output shows that the LCM of numbers 10, 15, and 20 is indeed 60.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "Great! I'm glad the code worked as expected and the output is correct. If you have any more questions or need further assistance, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCTO\u001b[0m (to user_proxy):\n",
      "\n",
      "Thank you! I'm glad the code worked for you. If you have any more questions or need further assistance in the future, don't hesitate to ask. I'm here to help!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to CTO):\n",
      "\n",
      "You're welcome! I'm glad I could help. Don't hesitate to reach out if you have any more questions or need assistance in the future. Have a great day!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-01 12:00:46] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\shahi\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-16k in organization org-esPZ8nZb9VNyw9mXukpQUoy9 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': 'Enter your key'\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"CTO\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Chief technical officer of a tech company\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\"work_dir\": \"web\"},\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\"\n",
    ")\n",
    "\n",
    "task = \"\"\"\n",
    "Write python code to output GCD of numbers 10, 15 and 20 , and then store the code in a file\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=task\n",
    ")\n",
    "\n",
    "task2 = \"\"\"\n",
    "Change the code in the file you just created to instead output LCM OF numbers 10,15 and 20\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=task2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeec7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
